{
  "header": {
    "title": "√Årbol de Decisi√≥n",
    "home": "Inicio",
    "language": "üåê Idioma",
    "theme": {
      "dark": "üåô Oscuro",
      "light": "‚òÄÔ∏è Claro"
    }
  },
  "hero": {
    "title": "Explorando el mundo de los √Årboles de Decisi√≥n",
    "subtitle": "Una introducci√≥n visual al aprendizaje autom√°tico"
  },
  "section-understanding": {
    "title": "Entendiendo √Årboles de Decisi√≥n y Poda",
    "intro": {
      "welcome": "Bienvenido a nuestra {guide} ‚Äî una de las herramientas m√°s {powerful} en el campo de la inteligencia artificial y la ciencia de datos.",
      "guide": "gu√≠a interactiva sobre √°rboles de decisi√≥n",
      "powerful": "intuitivas y poderosas",
      "purpose": "Este sitio fue creado para ayudarte a comprender, de forma {visual}, c√≥mo funcionan estas estructuras, c√≥mo se construyen, y por qu√© son tan √∫tiles para resolver problemas de {classification} y {regression}.",
      "visual": "visual y accesible",
      "classification": "clasificaci√≥n",
      "regression": "regresi√≥n",
      "pruning": "Adem√°s, exploraremos un concepto esencial para mejorar el rendimiento de los √°rboles: {pruningConcept}. Reduce la complejidad del modelo, evita el {overfitting} (sobreajuste) y hace que las predicciones sean m√°s confiables con datos nuevos.",
      "pruningConcept": "la poda",
      "overfitting": "overfitting",
      "conclusion": "Aqu√≠ encontrar√°s {clear}, {visual_examples} y {demonstrations} para construir y optimizar √°rboles de decisi√≥n. Ya seas principiante o tengas experiencia, nuestro objetivo es hacer que estos conceptos sean simples, interactivos y aplicables.",
      "clear": "explicaciones claras",
      "visual_examples": "ejemplos visuales",
      "demonstrations": "demostraciones paso a paso"
    }
  },
  "section-what-is": {
    "title": "¬øQu√© es?",
    "definition": "Un {decision_tree} es un modelo predictivo que representa una serie de decisiones estructuradas en forma de √°rbol, donde cada {internal_node} corresponde a una pregunta o condici√≥n basada en las caracter√≠sticas de los datos, y cada {branch} lleva a una posible respuesta o divisi√≥n. Las {leaves} del √°rbol indican el resultado final, como una clase o valor num√©rico. Este modelo es ampliamente utilizado en tareas de {classification} y {regression} debido a su interpretaci√≥n intuitiva: simula un proceso de toma de decisiones secuencial, donde seguimos un camino l√≥gico basado en las entradas hasta llegar a una conclusi√≥n.",
    "decision_tree": "√°rbol de decisi√≥n",
    "internal_node": "nodo interno",
    "branch": "rama",
    "leaves": "hojas",
    "classification": "clasificaci√≥n",
    "regression": "regresi√≥n"
  },
  "section-prediction": {
    "title": "C√≥mo funciona la predicci√≥n en un √°rbol de decisi√≥n",
    "description": "Durante el proceso de {prediction}, el √°rbol de decisi√≥n recibe una nueva entrada (o muestra) y la recorre desde el {root_to_leaf}, siguiendo las condiciones definidas en cada punto de decisi√≥n. En cada etapa, analiza el valor de una {feature} de la entrada y elige la {corresponding_branch} bas√°ndose en la condici√≥n (por ejemplo, si el valor es menor o mayor que un cierto l√≠mite). Este camino es {unique} y lleva directamente hasta una {leaf}, donde se registra la clase predicha (en clasificadores) o un valor num√©rico (en modelos de regresi√≥n). El proceso es {fast_interpretable}, funcionando como una secuencia l√≥gica de decisiones que culmina en una conclusi√≥n final.",
    "prediction": "predicci√≥n",
    "root_to_leaf": "nodo ra√≠z hasta una hoja",
    "feature": "caracter√≠stica",
    "corresponding_branch": "rama correspondiente",
    "unique": "√∫nico",
    "leaf": "hoja",
    "fast_interpretable": "r√°pido, directo e interpretable"
  },
  "section-gini": {
    "title": "¬øC√≥mo se eligen los cortes?",
    "description": "En un √°rbol de decisi√≥n para clasificaci√≥n binaria, los cortes se eligen bas√°ndose en la {purity} de las regiones creadas despu√©s de cada divisi√≥n. El algoritmo busca, de forma codiciosa, la mejor divisi√≥n en ese momento ‚Äî es decir, elige la caracter√≠stica y el punto de corte que m√°s aumenten la pureza de los subconjuntos generados.",
    "purity": "pureza",
    "purity_explanation": "La pureza de una regi√≥n es una medida de qu√© tan homog√©neas son las clases dentro de ella. Si todos los ejemplos en una regi√≥n pertenecen a la misma clase, esa regi√≥n se considera pura. Para cuantificar esta pureza, usamos m√©tricas como la {entropy} y el {gini_index}.",
    "entropy": "Entrop√≠a",
    "gini_index": "√çndice Gini",
    "metrics_behavior": "Estas m√©tricas alcanzan sus valores m√≠nimos (cero) cuando todos los ejemplos son de la misma clase (proporci√≥n 0 o 1), y valores m√°ximos cuando hay equilibrio entre las clases (proporci√≥n 0.5), es decir, cuando la incertidumbre es mayor. El algoritmo calcula la ganancia de pureza antes y despu√©s de cada posible divisi√≥n ‚Äî llamada {information_gain} (con entrop√≠a) o {gini_gain} ‚Äî y elige el corte que m√°s aumenta esta ganancia.",
    "information_gain": "Ganancia de Informaci√≥n",
    "gini_gain": "Ganancia de Gini",
    "process_conclusion": "Este proceso se repite recursivamente, creando una segmentaci√≥n binaria del espacio, hasta que se alcanza un criterio de parada, como el n√∫mero m√≠nimo de puntos por regi√≥n.",
    "interaction_hint": "Pasa el mouse sobre el gr√°fico para ver los valores espec√≠ficos de entrop√≠a y Gini para cada proporci√≥n de clase positiva.",
    "chart_labels": {
      "x_axis": "Proporci√≥n de la clase positiva (p)",
      "y_axis": "Pureza (Entrop√≠a o Gini)",
      "entropy_legend": "Entrop√≠a",
      "gini_legend": "Gini",
      "entropy_tooltip": "Entrop√≠a: {value}",
      "gini_tooltip": "Gini: {value}"
    }
  },
  "cutoffs": {
    "feature_label": "Caracter√≠stica {number}",
    "characteristic": "Caracter√≠stica:",
    "value_range": "Rango de valores:",
    "cutoff_point": "Punto de corte:",
    "loading": "Cargando...",
    "loading_data": "Cargando datos...",
    "error_loading": "‚ùå Error al cargar los datos",
    "city_comparison": "Sacramento (p√∫rpura) vs San Francisco (verde)",
    "steps": {
      "total_rooms": {
        "title": "Total de Habitaciones",
        "description": "Esta caracter√≠stica representa el n√∫mero total de habitaciones en cada propiedad. Es una caracter√≠stica fundamental que captura el tama√±o de la propiedad. Las propiedades con m√°s habitaciones tienden a tener caracter√≠sticas diferentes y esta informaci√≥n es valiosa para el modelo. Observe c√≥mo se distribuye esta caracter√≠stica en los datos y c√≥mo establecemos un punto de corte para binarizar esta caracter√≠stica continua.",
        "axis_label": "Total de Habitaciones"
      },
      "total_bedrooms": {
        "title": "Dormitorios",
        "description": "Esta caracter√≠stica captura espec√≠ficamente los dormitorios, diferenci√°ndose de la caracter√≠stica anterior que incluye todos los tipos de habitaciones. Es una caracter√≠stica importante para determinar la capacidad habitacional de la propiedad. Esta caracter√≠stica proporciona informaci√≥n espec√≠fica sobre la funcionalidad residencial del inmueble.",
        "axis_label": "Total de Dormitorios"
      },
      "households": {
        "title": "Densidad de Hogares",
        "description": "Esta caracter√≠stica representa la cantidad de hogares por regi√≥n, capturando aspectos de densidad poblacional y urbanizaci√≥n. Es una caracter√≠stica contextual importante que refleja el ambiente donde se encuentra ubicada la propiedad. Esta informaci√≥n demogr√°fica es valiosa para caracterizar el tipo de √°rea.",
        "axis_label": "N√∫mero de Hogares"
      },
      "median_house_value": {
        "title": "Valor Mediano de las Casas",
        "description": "Esta caracter√≠stica representa el valor mediano de las casas en la regi√≥n. Es una caracter√≠stica econ√≥mica importante que refleja el mercado inmobiliario local. Esta caracter√≠stica captura informaci√≥n sobre el poder adquisitivo y patr√≥n socioecon√≥mico del √°rea donde se encuentra ubicada la propiedad.",
        "axis_label": "Valor Mediano (USD)"
      }
    }
  },
  "scatterplot": {
    "frequency": "Frecuencia",
    "value": "Valor",
    "cutoff_tooltip": "valor < {value}",
    "point_tooltip": {
      "property_of": "Propiedad de {city}",
      "value": "Valor: {value}",
      "frequency": "Frecuencia: ~{frequency} propiedades similares",
      "below_cutoff": "Debajo del punto de corte",
      "above_cutoff": "Encima del punto de corte"
    }
  },
  "page": {
    "testTitle": "P√°gina de Prueba",
    "step": "Paso {number}",
    "stage": "Etapa {number}"
  },
  "scroll": {
    "block1": {
      "title": "Etapa {number}",
      "content": "Texto para la etapa {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    },
    "block2": {
      "title": "Bloque 2 - Etapa {number}",
      "content": "Contenido diferente para el bloque 2 - etapa {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    }
  },
  "section-tree-creation": {
    "title": "Creaci√≥n del √Årbol y Cortes en el Espacio de Datos",
    "description": "La creaci√≥n del √°rbol de decisi√≥n consiste en dividir repetidamente el espacio de los datos en regiones m√°s peque√±as por medio de {cuts} sobre las caracter√≠sticas. Cada corte separa los datos en grupos m√°s homog√©neos, facilitando la toma de decisiones. Este proceso contin√∫a hasta que las regiones est√©n suficientemente puras o se alcance un criterio de parada, resultando en una estructura jer√°rquica que refleja estas divisiones en el espacio.",
    "cuts": "cortes basados en condiciones"
  },
  "section-interactive-prediction": {
    "title": "C√≥mo funciona la predicci√≥n en un √°rbol de decisi√≥n",
    "practice_text": "Para ver esto en la pr√°ctica, elige un valor para cada variable y observa cu√°l ser√° la predicci√≥n en este √°rbol ya construido anteriormente:",
    "loading_tree": "Cargando √°rbol de decisi√≥n...",
    "interactive_placeholder": "Aqu√≠ ir√° un √°rbol interactivo, donde construimos un dato y vemos recorrer el √°rbol hasta la predicci√≥n"
  },
  "section-complete-tree": {
    "title": "El √Årbol Completo",
    "description": "Un √°rbol de decisi√≥n que crece completamente contin√∫a realizando divisiones en el espacio de los datos hasta que cada hoja contenga solo ejemplos de una √∫nica clase, alcanzando as√≠ {accuracy} en el conjunto de entrenamiento. Aunque este modelo memoriza perfectamente los datos, tiende a ser muy complejo y espec√≠fico, lo que puede llevar al {overfitting}, es decir, a una menor capacidad de generalizar para datos nuevos y no vistos.",
    "accuracy": "100% de precisi√≥n",
    "overfitting": "sobreajuste",
    "example_text": "Ve abajo c√≥mo quedar√≠a el √°rbol completo generado con todas las caracter√≠sticas de nuestros datos de ejemplo"
  },
  "footer": {
    "developed_by": "Desarrollado por Paula Eduarda de Lima, Mariana Fernandes Rocha y Joel Perca con SvelteKit & D3.js"
  }
}
