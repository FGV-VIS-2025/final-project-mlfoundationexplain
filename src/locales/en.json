{
  "header": {
    "title": "Decision Tree",
    "home": "Home",
    "language": "üåê Language",
    "theme": {
      "dark": "üåô Dark",
      "light": "‚òÄÔ∏è Light"
    }
  },
  "hero": {
    "title": "Exploring the World of Decision Trees",
    "subtitle": "A visual introduction to machine learning"
  },
  "section-understanding": {
    "title": "Understanding Decision Trees and Pruning",
    "intro": {
      "welcome": "Welcome to our {guide} ‚Äî one of the most {powerful} tools in the field of artificial intelligence and data science.",
      "guide": "interactive guide on decision trees",
      "powerful": "intuitive and powerful",
      "purpose": "This site was created to help you understand, in a {visual} way, how these structures work, how they are built, and why they are so useful for solving {classification} and {regression} problems.",
      "visual": "visual and accessible",
      "classification": "classification",
      "regression": "regression",
      "pruning": "Additionally, we will explore an essential concept for improving tree performance: {pruningConcept}. It reduces model complexity, avoids {overfitting}, and makes predictions more reliable with new data.",
      "pruningConcept": "pruning",
      "overfitting": "overfitting",
      "conclusion": "Here, you will find {clear}, {visual_examples}, and {demonstrations} to build and optimize decision trees. Whether you are a beginner or already experienced, our goal is to make these concepts simple, interactive, and applicable.",
      "clear": "clear explanations",
      "visual_examples": "visual examples",
      "demonstrations": "step-by-step demonstrations"
    }
  },
  "section-what-is": {
    "title": "What is it?",
    "definition": "A {decision_tree} is a predictive model that represents a series of decisions structured in tree form, where each {internal_node} corresponds to a question or condition based on data characteristics, and each {branch} leads to a possible answer or division. The {leaves} of the tree indicate the final result, such as a class or numerical value. This model is widely used in {classification} and {regression} tasks due to its intuitive interpretation: it simulates a sequential decision-making process, where we follow a logical path based on inputs until reaching a conclusion.",
    "decision_tree": "decision tree",
    "internal_node": "internal node",
    "branch": "branch",
    "leaves": "leaves",
    "classification": "classification",
    "regression": "regression"
  },

  "section-dataset": {
    "title": "About the Dataset used",
    "paragraph1": "The original dataset contains information about houses in specific districts of California, with summarized statistics based on the 1990 census data. For the specific purpose of our visualization ‚Äî which focuses on the <b>classification of houses between Sacramento and San Francisco</b> ‚Äî we selected and transformed variables. The goal was to focus on intrinsic property characteristics and location for the binary classification task.",
    "paragraph2": "Consequently, variables that did not directly contribute to describing property characteristics, or that became redundant for geographic classification after introducing the <code>city</code> variable, were removed.",
    "variables_intro": "The variables used in our visualization are:",
    "variables": {
      "total_rooms": "<b>total rooms</b>: Represents the total number of rooms within a block. This variable helps to understand the size or capacity of a property.",
      "total_bedrooms": "<b>total bedrooms</b>: Indicates the total number of bedrooms within a block. It complements <i>total rooms</i> by providing a more specific measure of the property's composition.",
      "households": "<b>households</b>: Corresponds to the total number of households residing in housing units within a block. It provides context about population density and property usage.",
      "median_house_value": "<b>median house value</b>: Refers to the median house value within a block, measured in US dollars. Although it is not the target variable for our classification, it is a relevant socioeconomic indicator that may correlate with location.",
      "city": "<b>city</b>: This is the target variable created for our classification problem. It indicates whether a house is located in <b>Sacramento</b> or <b>San Francisco</b>."
    },
    "paragraph3": "This selection allowed the visualization to focus on the relationships between property characteristics and geographic classification."
  },

  "section-prediction": {
    "title": "How prediction works in a decision tree",
    "description": "During the {prediction} process, the decision tree receives a new input (or sample) and traverses it from {root_to_leaf}, following the conditions defined at each decision point. At each step, it analyzes the value of a {feature} from the input and chooses the {corresponding_branch} based on the condition (for example, whether the value is less than or greater than a certain threshold). This path is {unique} and leads directly to a {leaf}, where the predicted class (in classifiers) or a numerical value (in regression models) is recorded. The process is {fast_interpretable}, functioning as a logical sequence of decisions that culminates in a final conclusion.",
    "prediction": "prediction",
    "root_to_leaf": "root node to a leaf",
    "feature": "feature",
    "corresponding_branch": "corresponding branch",
    "unique": "unique",
    "leaf": "leaf",
    "fast_interpretable": "fast, direct and interpretable"
  },
  "section-gini": {
    "title": "How are splits chosen?",
    "description": "In a decision tree for binary classification, splits are chosen based on the {purity} of regions created after each division.",
    "purity": "purity",
    "purity_explanation": "The purity of a region is a measure of how homogeneous the classes within it are. If all examples in a region belong to the same class, that region is considered pure. To quantify this purity, we use metrics such as {entropy} (H) and the {gini_index} (G).",
    "entropy": "Entropy",
    "gini_index": "Gini Index",
    "metrics_behavior": "These metrics reach their minimum values (zero) when all examples are from the same class (proportion 0 or 1), and maximum values when there is balance between classes (proportion 0.5), that is, when uncertainty is greatest. The algorithm calculates the purity gain before and after each possible split ‚Äî called {information_gain} (with entropy) or {gini_gain} ‚Äî and chooses the cut that most increases this gain.",
    "information_gain": "Information Gain",
    "gini_gain": "Gini Gain",
    "process_conclusion": "This process is repeated recursively, creating a binary segmentation of space, until a stopping criterion is reached, such as the minimum number of points per region.",
    "interaction_hint": "Hover over the chart to see specific entropy and Gini values for each positive class proportion.",
    "chart_labels": {
      "x_axis": "Positive class proportion (p)",
      "y_axis": "Purity (Entropy or Gini)",
      "entropy_legend": "Entropy",
      "gini_legend": "Gini",
      "entropy_tooltip": "Entropy: {value}",
      "gini_tooltip": "Gini: {value}"
    }
  },
  "cutoffs": {
    "title": "Individual Cutoff Visualization",
     "paragraph": "One way to view these cutoffs is by projecting onto a single variable to observe how the univariate distribution behaves and to cut at the point where the classes are best separated.",
    "paragraph2": "<p>The choice of the best cutoff point in a decision tree is made by evaluating, for each possible value of the variable, how the data split impacts the purity of the formed groups. In the graph above, the data distribution is shown as a histogram, separated by classes (San Francisco and Sacramento). The dashed line represents the selected cutoff point ‚Äî the one that generates the lowest <strong>weighted impurity</strong> between the two sides (left and right).<br><br>Mathematically, this choice is made by minimizing the following weighted impurity function:</p>",
    "paragraph3": "<p>Where:<br>‚Äì <strong>N<sub>left</sub></strong> and <strong>N<sub>right</sub></strong> are the number of samples in the left and right groups after the split.<br>‚Äì <strong>N<sub>total</sub></strong> is the total number of samples before the split.<br>‚Äì <strong>Impurity<sub>left</sub></strong> and <strong>Impurity<sub>right</sub></strong> are the impurities calculated on each side, which can be measured by <em>Gini Index</em>, <em>Entropy</em>, or another metric.<br><br>Below the histogram, the impurity graphs indicate how it varies across the possible cutoff values: the left and right impurity lines show how homogeneous the groups are after each possible division. The chosen cutoff point is where the weighted impurity reaches its lowest value, seeking the most efficient separation between the classes.<br>Modify the cut to see the impurity change:</p>",
    "feature_label": "Feature {number}",
    "characteristic": "Characteristic:",
    "value_range": "Value range:",
    "cutoff_point": "Cutoff point:",
    "loading": "Loading...",
    "loading_data": "Loading data...",
    "error_loading": "‚ùå Error loading data",
    "city_comparison": "Sacramento (purple) vs San Francisco (green)",
    "steps": {
      "total_rooms": {
        "title": "Total Rooms",
        "description": "This feature represents the total number of rooms across all households in a block. It is a metric that reflects, in aggregate, the size of properties in that area. Blocks with more rooms generally indicate larger homes or a higher housing density. This characteristic is important for the model as it helps identify patterns in the housing market. In the chart, you can observe how this variable is distributed and how we define a cutoff point that best separates the classes, turning this continuous feature into a more interpretable binary split.",
        "axis_label": "Total Rooms"
      },
      "total_bedrooms": {
        "title": "Total Bedrooms",
        "description": "This feature indicates the total number of bedrooms within a block. It complements 'Total Rooms' by providing a more specific measure of how the properties are composed. It is an important characteristic for determining the residential capacity of the properties. This feature offers valuable information about the functional living space of the homes.",
        "axis_label": "Total Bedrooms"

      },
      "households": {
        "title": "Total Households",
        "description": "Indicates the total number of households ‚Äî defined as groups of people residing within a single housing unit ‚Äî in a block. This variable captures the residential structure of the area, serving as an important indicator of population density and neighborhood characteristics. Blocks with higher household counts may reflect more densely populated or subdivided residential areas.",
        "axis_label": "Total Households"

      },
      "median_house_value": {
        "title": "Median House Value",
        "description": "This feature represents the median value of houses in the region. It's an important economic characteristic that reflects the local real estate market. This feature captures information about purchasing power and socioeconomic patterns of the area where the property is located.",
        "axis_label": "Median Value (USD)"
      }
    }
  },
  "scatterplot": {
    "frequency": "Frequency",
    "value": "Value",
    "cutoff_tooltip": "value < {value}",
    "point_tooltip": {
      "property_of": "Property from {city}",
      "value": "Value: {value}",
      "frequency": "Frequency: ~{frequency} similar properties",
      "below_cutoff": "Below cutoff point",
      "above_cutoff": "Above cutoff point"
    }
  },
  "page": {
    "testTitle": "Test Page",
    "step": "Step {number}",
    "stage": "Stage {number}"
  },
  "scroll": {
    "block1": {
      "title": "Stage {number}",
      "content": "Text for stage {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    },
    "block2": {
      "title": "Block 2 - Stage {number}",
      "content": "Different content for block 2 - stage {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    }
  },
  "section-tree-creation": {
    "title": "Tree Creation and Data Space Cuts",
    "description": "Creating a decision tree consists of repeatedly dividing the data space into smaller regions through {cuts} on features. Each cut separates the data into more homogeneous groups, facilitating decision-making. This process continues until the regions are sufficiently pure or a stopping criterion is reached, resulting in a hierarchical structure that reflects these divisions in space.",
    "cuts": "condition-based cuts"
  },
  "section-interactive-prediction": {
    "title": "How prediction works in a decision tree",
    "practice_text": "To see this in practice, choose a value for each variable and see what the prediction will be in this previously built tree:",
    "loading_tree": "Loading decision tree...",
    "interactive_placeholder": "Here will be an interactive tree, where we build data and see it traverse the tree to the prediction"
  },
  "section-complete-tree": {
    "title": "The Complete Tree",
    "description": "A decision tree that grows completely continues making divisions in the data space until each leaf contains only examples from a single class, thus achieving {accuracy} on the training set. Although this model perfectly memorizes the data, it tends to be very complex and specific, which can lead to {overfitting}, that is, a reduced ability to generalize to new and unseen data.",
    "accuracy": "100% accuracy",
    "overfitting": "overfitting",
    "example_text": "See below how the complete tree generated with all features from our example data would look"
  },
  "pruning": {
    "title": "Decision Tree Pruning Methods",
    "description": "Select a pruning method to see how it affects the decision tree structure",
    "selected_method": "Selected Method:",
    "loading": "Loading decision tree...",
    "error": "Error loading tree. Please try selecting another pruning method.",
    "accuracy": "Accuracy:",
    "nodes": "Nodes:",
    "depth": "Depth:",
    "methods": {
      "original": {
        "name": "No Pruning",
        "description": "Complete tree without any pruning applied"
      },
      "validacao": {
        "name": "Validation Pruning",
        "description": "Pruning based on validation accuracy improvement"
      },
      "profundidade_3": {
        "name": "Depth Pruning (3)",
        "description": "Limits maximum depth to 3 levels"
      },
      "profundidade_4": {
        "name": "Depth Pruning (4)",
        "description": "Limits maximum depth to 4 levels"
      },
      "custo_complexidade_001": {
        "name": "Cost-Complexity (Œ±=0.01)",
        "description": "Cost-complexity pruning with Œ±=0.01"
      },
      "hibrida": {
        "name": "Hybrid Pruning",
        "description": "Combines multiple pruning criteria"
      }
    }
  },
  "footer": {
    "developed_by": "Developed by Paula Eduarda de Lima, Mariana Fernandes Rocha and Joel Perca with SvelteKit & D3.js"
  },
  "step3": {
    "steps":[
        {
          "title": "No Splits",
          "content": "The tree classifies all regions according to the most frequent class. Before any splits, the classification would be 'San Francisco', which is the most common class in the data."
        },
        {
          "title": "Depth 0 ‚Äî Tree Root",
          "content": "At the top of the tree, we have the first decision based on the condition <code>feature 1 ‚â§ 185300</code>. This split separates the data into two large groups. The colors indicate the class assigned to each respective region."
        },
        {
          "title": "Depth 1 ‚Äî First Branch Split",
          "content": "At this level, each group generated by the previous split is analyzed separately. The tree applies new splits within each group, creating more specific subdivisions. This process reflects the recursive nature of decision trees: each branch is treated as a new subproblem, where the algorithm searches locally for the best split to reduce impurity."
        },
        {
          "title": "Depth 2 ‚Äî Detailed Branching",
          "content": "Next, new splits appear, such as <code>feature 1 ‚â§ 72600</code>. The algorithm continues testing all possible split points across all available variables and selects the one that provides the highest information gain. This gain is calculated based on an impurity metric, such as entropy or Gini index, which will be explored in the next steps."
        },
        {
          "title": "Depth 3 ‚Äî Orthogonal Splits",
          "content": "Decision trees perform orthogonal splits in the feature space, meaning each cut considers only one variable at a time, drawing a straight line (or a plane in higher dimensions) perpendicular to the chosen feature axis. This creates decision boundaries in the form of rectangles in 2D space or hyper-rectangles (hypercubes) in higher dimensions. This partitioning works well for data with axis-aligned patterns but can be limiting when the natural boundaries are curved or diagonal."
        },
        {
          "title": "Depth 4 ‚Äî Stability",
          "content": "Decision trees are high-variance models: small changes in the dataset can lead to completely different trees. This is why ensemble techniques like Random Forests or Boosting were developed to improve robustness and reduce variance."
        },
        {
          "title": "Depth 5 ‚Äî Tree Leaves",
          "content": "The leaves of the tree correspond to the model's final predictions: \"Sacramento\" or \"San Francisco.\" Each leaf represents a region of the feature space where the model makes a decision. Visually, the color of each rectangle indicates the class assigned to that region. Any point that falls within one of these areas will be classified according to the corresponding color, reflecting the majority class in that subdivision."
        },
        {
          "title": "Depth 6 ‚Äî Interesting Facts",
          "content": "Decision trees are not limited to classification tasks; they can also be used for regression. In regression trees, the predicted value at each leaf is the average of the target values of the samples in that node. The splitting criterion, in this case, is often the mean squared error (MSE)."
        },
        {
          "title": "Depth 7 ‚Äî Visual Interpretability",
          "content": "The hierarchical structure of a decision tree is highly interpretable, making it ideal for educational, legal, medical, and business applications where decisions must be explainable and transparent."
        },
        {
          "title": "Depth 8 ‚Äî Recursive Growth",
          "content": "The construction of a decision tree follows a recursive process of splitting. At each step, the algorithm considers only the data that reached that node and chooses the best split based on an impurity metric. This allows each branch to specialize in a specific part of the feature space, progressively adapting to the observed patterns. This growth continues until one or more stopping criteria are met, such as maximum depth, minimum number of samples per node, or full purity in the leaves."
        },
        {
          "title": "Depth 9 ‚Äî Stopping Criteria",
          "content": "Tree growth is not infinite. It stops when one or more stopping criteria are met. This can occur when: (1) the node becomes pure, meaning all samples belong to the same class; (2) the number of samples in the node is smaller than a predefined minimum; (3) the tree reaches the maximum allowed depth; or (4) there is no significant information gain from further splits. These criteria control the complexity of the model and help prevent overfitting."
        },
        {
          "title": "Depth 10 ‚Äî Overfitting",
          "content": "If no constraints are applied, the tree will continue splitting until each leaf contains only one sample, achieving 100% accuracy on the training data. However, this usually leads to overfitting, where the model fits the training data too closely and loses its ability to generalize to unseen data. This is why techniques like pruning are essential to limit complexity and improve predictive performance."
        }
      ]
      ,
    "step_not_available": "Step {number} not available."
  },
  "about": {
    "title": "About",
    "presentation-title": "Presentation",
    "presentation-text": "On the Home tab, you will find our interactive visualization that guides the explanation of how the decision tree algorithm works. In addition to showing the tree structure, we offer dynamic visualizations that help understand the tree-building process, allowing comprehension of how the splits are chosen.",
    "resume-title": "Project Summary",
    "resume-text": "The platform presents ",
    "modules": "visual and interactive modules ",
    "resume-text2": "that facilitate the understanding of ",
    "images": "Model views",
    "models": "models based on binary trees.",
    "list-title": "Main features include:",
    "item1": "Dynamic construction of the decision tree along with the cuts in the 2D data space",
    "item2": "Interactive prediction simulations",
    "item3": "Impurity metric charts (Gini Index and Entropy)",
    "item4": "Histograms illustrating univariate splits",
    "item5": "Dendrogram illustrating manual pruning of complex trees",
    "accessibility": "The project was also developed with a focus on accessibility and customization, including:",
    "acc-list1": "Multilingual support (Portuguese, Spanish, and English)",
    "acc-list2": "Toggle between light and dark themes",
    "article-title": "Article",
    "click": "Click here",
    "article-text": " to access the full article",
    "video-title": "Presentation Video",
    "team-members": "Team Members",
    "instructions-title": "Execution Instructions",
    "instructions-text": "This project was created using",
    "local-run": ". To run locally:",
    "run-step1": "Clone the repository available via the 'GitHub' tab on this page.",
    "run-step2": "Install dependencies with ",
    "run-step3": "Run the development server with ",
    "run-step4": "Access in the browser",
    "run-step5": "(or the port shown in the terminal)"
  }
}
