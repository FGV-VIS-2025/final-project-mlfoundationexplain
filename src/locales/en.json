{
  "header": {
    "title": "Decision Tree",
    "home": "Home",
    "language": "üåê Language",
    "theme": {
      "dark": "üåô Dark",
      "light": "‚òÄÔ∏è Light"
    }
  },
  "hero": {
    "title": "Exploring the World of Decision Trees",
    "subtitle": "A visual introduction to machine learning"
  },
  "section-understanding": {
    "title": "Understanding Decision Trees and Pruning",
    "intro": {
      "welcome": "Welcome to our {guide} ‚Äî one of the most {powerful} tools in the field of artificial intelligence and data science.",
      "guide": "interactive guide on decision trees",
      "powerful": "intuitive and powerful",
      "purpose": "This site was created to help you understand, in a {visual} way, how these structures work, how they are built, and why they are so useful for solving {classification} and {regression} problems.",
      "visual": "visual and accessible",
      "classification": "classification",
      "regression": "regression",
      "pruning": "Additionally, we will explore an essential concept for improving tree performance: {pruningConcept}. It reduces model complexity, avoids {overfitting}, and makes predictions more reliable with new data.",
      "pruningConcept": "pruning",
      "overfitting": "overfitting",
      "conclusion": "Here, you will find {clear}, {visual_examples}, and {demonstrations} to build and optimize decision trees. Whether you are a beginner or already experienced, our goal is to make these concepts simple, interactive, and applicable.",
      "clear": "clear explanations",
      "visual_examples": "visual examples",
      "demonstrations": "step-by-step demonstrations"
    }
  },
  "section-what-is": {
    "title": "What is it?",
    "definition": "A {decision_tree} is a predictive model that represents a series of decisions structured in tree form, where each {internal_node} corresponds to a question or condition based on data characteristics, and each {branch} leads to a possible answer or division. The {leaves} of the tree indicate the final result, such as a class or numerical value. This model is widely used in {classification} and {regression} tasks due to its intuitive interpretation: it simulates a sequential decision-making process, where we follow a logical path based on inputs until reaching a conclusion.",
    "decision_tree": "decision tree",
    "internal_node": "internal node",
    "branch": "branch",
    "leaves": "leaves",
    "classification": "classification",
    "regression": "regression"
  },

  "section-dataset": {
    "title": "About the Dataset",
    "paragraph1": "The original dataset contains information about houses in specific districts of California, with summarized statistics based on the 1990 census data. For the specific purpose of our visualization ‚Äî which focuses on the <b>classification of houses between Sacramento and San Francisco</b> ‚Äî we selected and transformed variables. The goal was to focus on intrinsic property characteristics and location for the binary classification task.",
    "paragraph2": "Consequently, variables that did not directly contribute to describing property characteristics, or that became redundant for geographic classification after introducing the <code>city</code> variable, were removed.",
    "variables_intro": "The variables used in our visualization are:",
    "variables": {
      "total_rooms": "<b>total rooms</b>: Represents the total number of rooms within a block. This variable helps to understand the size or capacity of a property.",
      "total_bedrooms": "<b>total bedrooms</b>: Indicates the total number of bedrooms within a block. It complements <i>total rooms</i> by providing a more specific measure of the property's composition.",
      "households": "<b>households</b>: Corresponds to the total number of households residing in housing units within a block. It provides context about population density and property usage.",
      "median_house_value": "<b>median house value</b>: Refers to the median house value within a block, measured in US dollars. Although it is not the target variable for our classification, it is a relevant socioeconomic indicator that may correlate with location.",
      "city": "<b>city</b>: This is the target variable created for our classification problem. It indicates whether a house is located in <b>Sacramento</b> or <b>San Francisco</b>."
    },
    "paragraph3": "This selection allowed the visualization to focus on the relationships between property characteristics and geographic classification."
  },



  "section-prediction": {
    "title": "How prediction works in a decision tree",
    "description": "During the {prediction} process, the decision tree receives a new input (or sample) and traverses it from {root_to_leaf}, following the conditions defined at each decision point. At each step, it analyzes the value of a {feature} from the input and chooses the {corresponding_branch} based on the condition (for example, whether the value is less than or greater than a certain threshold). This path is {unique} and leads directly to a {leaf}, where the predicted class (in classifiers) or a numerical value (in regression models) is recorded. The process is {fast_interpretable}, functioning as a logical sequence of decisions that culminates in a final conclusion.",
    "prediction": "prediction",
    "root_to_leaf": "root node to a leaf",
    "feature": "feature",
    "corresponding_branch": "corresponding branch",
    "unique": "unique",
    "leaf": "leaf",
    "fast_interpretable": "fast, direct and interpretable"
  },
  "section-gini": {
    "title": "How are splits chosen?",
    "description": "In a decision tree for binary classification, splits are chosen based on the {purity} of regions created after each division.",
    "purity": "purity",
    "purity_explanation": "The purity of a region is a measure of how homogeneous the classes within it are. If all examples in a region belong to the same class, that region is considered pure. To quantify this purity, we use metrics such as {entropy} (H) and the {gini_index} (G).",
    "entropy": "Entropy",
    "gini_index": "Gini Index",
    "metrics_behavior": "These metrics reach their minimum values (zero) when all examples are from the same class (proportion 0 or 1), and maximum values when there is balance between classes (proportion 0.5), that is, when uncertainty is greatest. The algorithm calculates the purity gain before and after each possible split ‚Äî called {information_gain} (with entropy) or {gini_gain} ‚Äî and chooses the cut that most increases this gain.",
    "information_gain": "Information Gain",
    "gini_gain": "Gini Gain",
    "process_conclusion": "This process is repeated recursively, creating a binary segmentation of space, until a stopping criterion is reached, such as the minimum number of points per region.",
    "interaction_hint": "Hover over the chart to see specific entropy and Gini values for each positive class proportion.",
    "chart_labels": {
      "x_axis": "Positive class proportion (p)",
      "y_axis": "Purity (Entropy or Gini)",
      "entropy_legend": "Entropy",
      "gini_legend": "Gini",
      "entropy_tooltip": "Entropy: {value}",
      "gini_tooltip": "Gini: {value}"
    }
  },
  "cutoffs": {
    "title": "Individual Cutoff Visualization",
    "paragraph": "One way to visualize these splits is by projecting onto a single variable, to observe how the univariate distribution occurs and determine the point that best separates the classes. Below are some examples:",
    "feature_label": "Feature {number}",
    "characteristic": "Characteristic:",
    "value_range": "Value range:",
    "cutoff_point": "Cutoff point:",
    "loading": "Loading...",
    "loading_data": "Loading data...",
    "error_loading": "‚ùå Error loading data",
    "city_comparison": "Sacramento (purple) vs San Francisco (green)",
    "steps": {
      "total_rooms": {
        "title": "Total Rooms",
        "description": "This feature represents the total number of rooms in each property. It's a fundamental characteristic that captures the size of the property. Properties with more rooms tend to have different characteristics and this information is valuable for the model. Observe how this feature is distributed in the data and how we establish a cutoff point to binarize this continuous characteristic.",
        "axis_label": "Total Rooms"
      },
      "total_bedrooms": {
        "title": "Total Bedrooms",
        "description": "This feature specifically captures bedrooms, differing from the previous feature that includes all types of rooms. It's an important characteristic for determining the residential capacity of the property. This feature provides specific information about the residential functionality of the property.",
        "axis_label": "Total Bedrooms"
      },
      "households": {
        "title": "Household Density",
        "description": "This feature represents the number of households per region, capturing aspects of population density and urbanization. It's an important contextual characteristic that reflects the environment where the property is located. This demographic information is valuable for characterizing the type of area.",
        "axis_label": "Number of Households"
      },
      "median_house_value": {
        "title": "Median House Value",
        "description": "This feature represents the median value of houses in the region. It's an important economic characteristic that reflects the local real estate market. This feature captures information about purchasing power and socioeconomic patterns of the area where the property is located.",
        "axis_label": "Median Value (USD)"
      }
    }
  },
  "scatterplot": {
    "frequency": "Frequency",
    "value": "Value",
    "cutoff_tooltip": "value < {value}",
    "point_tooltip": {
      "property_of": "Property from {city}",
      "value": "Value: {value}",
      "frequency": "Frequency: ~{frequency} similar properties",
      "below_cutoff": "Below cutoff point",
      "above_cutoff": "Above cutoff point"
    }
  },
  "page": {
    "testTitle": "Test Page",
    "step": "Step {number}",
    "stage": "Stage {number}"
  },
  "scroll": {
    "block1": {
      "title": "Stage {number}",
      "content": "Text for stage {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    },
    "block2": {
      "title": "Block 2 - Stage {number}",
      "content": "Different content for block 2 - stage {number}. Lorem ipsum dolor sit amet consectetur adipisicing elit."
    }
  },
  "section-tree-creation": {
    "title": "Tree Creation and Data Space Cuts",
    "description": "Creating a decision tree consists of repeatedly dividing the data space into smaller regions through {cuts} on features. Each cut separates the data into more homogeneous groups, facilitating decision-making. This process continues until the regions are sufficiently pure or a stopping criterion is reached, resulting in a hierarchical structure that reflects these divisions in space.",
    "cuts": "condition-based cuts"
  },
  "section-interactive-prediction": {
    "title": "How prediction works in a decision tree",
    "practice_text": "To see this in practice, choose a value for each variable and see what the prediction will be in this previously built tree:",
    "loading_tree": "Loading decision tree...",
    "interactive_placeholder": "Here will be an interactive tree, where we build data and see it traverse the tree to the prediction"
  },
  "section-complete-tree": {
    "title": "The Complete Tree",
    "description": "A decision tree that grows completely continues making divisions in the data space until each leaf contains only examples from a single class, thus achieving {accuracy} on the training set. Although this model perfectly memorizes the data, it tends to be very complex and specific, which can lead to {overfitting}, that is, a reduced ability to generalize to new and unseen data.",
    "accuracy": "100% accuracy",
    "overfitting": "overfitting",
    "example_text": "See below how the complete tree generated with all features from our example data would look"
  },
  "footer": {
    "developed_by": "Developed by Paula Eduarda de Lima, Mariana Fernandes Rocha and Joel Perca with SvelteKit & D3.js"
  },
  "step3": {
    "steps": [
      {
        "title": "Depth 0 0 Tree Root",
        "content": "At the top of the tree, we have the first decision based on the condition <code>feature 1 ‚â§ 185300</code>. This split separates the data into two large groups..."
      },
      {
        "title": "Depth 1 - First Branch Division",
        "content": "The tree now splits into two main paths. On the left side..."
      },
      {
        "title": "Depth 2 - Detailed Ramifications",
        "content": "Next, new divisions emerge, such as <code>feature 1 ‚â§ 72600</code>..."
      },
      {
        "title": "Depth 3 - Very Specific Conditions",
        "content": "At this level, we have more specific divisions, such as <code>feature 0 ‚â§ 448</code>..."
      },
      {
        "title": "Depth 4 - Tree Leaves",
        "content": "The tree leaves represent the final predictions: \"Sacramento\" or \"San Francisco\"..."
      }
    ],
    "step_not_available": "Step {number} not available."
  },
  "about": {
    "title": "About",
    "presentation-title": "Presentation",
    "presentation-text": "On the Home tab, you will find our interactive visualization that guides the explanation of how the decision tree algorithm works. In addition to showing the tree structure, we offer dynamic visualizations that help understand the tree-building process, allowing comprehension of how the splits are chosen.",
    "resume-title": "Project Summary",
    "resume-text": "The platform presents ",
    "modules": "visual and interactive modules ",
    "resume-text2": "that facilitate the understanding of ",
    "images": "Model views",
    "models": "models based on binary trees.",
    "list-title": "Main features include:",
    "item1": "Dynamic construction of the decision tree along with the cuts in the 2D data space",
    "item2": "Interactive prediction simulations",
    "item3": "Impurity metric charts (Gini Index and Entropy)",
    "item4": "Histograms illustrating univariate splits",
    "item5": "Dendrogram illustrating manual pruning of complex trees",
    "accessibility": "The project was also developed with a focus on accessibility and customization, including:",
    "acc-list1": "Multilingual support (Portuguese, Spanish, and English)",
    "acc-list2": "Toggle between light and dark themes",
    "article-title": "Article",
    "click": "Click here",
    "article-text": " to access the full article",
    "video-title": "Presentation Video",
    "team-members": "Team Members",
    "instructions-title": "Execution Instructions",
    "instructions-text": "This project was created using",
    "local-run": ". To run locally:",
    "run-step1": "Clone the repository available via the 'GitHub' tab on this page.",
    "run-step2": "Install dependencies with ",
    "run-step3": "Run the development server with ",
    "run-step4": "Access in the browser",
    "run-step5": "(or the port shown in the terminal)"
  }
}
